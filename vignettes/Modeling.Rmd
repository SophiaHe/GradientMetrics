---
title: "Modeling"
output: 
  rmarkdown::html_vignette:
   self_contained: yes
   mode: selfcontained
vignette: >
  %\VignetteIndexEntry{Modeling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(GradientMatricsSubmission)
# library(dplyr)
library(tidyverse)
library(MASS)
library(ordinal)
# library(glmmLasso)
library(highcharter)
library(nnet)
library(cvms)
library(ggeffects)
```

## Data Processing

The goal of the modeling process is to answer the question: how each attribute (and its level) affects the likelihood of downloading of the app. In other words, I need to build statistical models that model the effects of each independent variables (a.k.a. each attribute) on the response variable (paricipant's lieklihood of download, on a rating-based scale). 
 
```{r}
data("survey_data") 
data("experiment_data")
table(exp_data$answer)

model_data = exp_data %>% 
  mutate(
    response_id = as.factor(response_id),
    answer = ordered(answer, levels = c("1", "2", "3","4")),
    offer = as.factor(offer),
    outcome = as.factor(outcome),
    rtb= as.factor(rtb),
    social_proof = as.factor(social_proof),
    duration = factor(duration,levels = c("3 months","6 months","12 months")),
    price = factor(price,levels = c("$20/month","$30/month","$40/month"))
  ) %>% 
  dplyr::select(-c(task)) #response_id

# merge categories in response variable
# model_data2 = model_data %>% 
#   mutate(answer = case_when(
#     answer == 1 ~ 1,
#     answer %in% c(2,3) ~ 2,
#     answer == 4 ~ 3
#   ),
#   answer =  ordered(answer, levels = c("1", "2", "3")),
#   duration = factor(duration,levels = c("3 months","6 months","12 months")),
#   price = factor(price,levels = c("$20/month","$30/month","$40/month"))
#   )
# 
# table(model_data2$answer)
# # 3 levels
IDs = unique(model_data$response_id)
set.seed(123)
train_model_data = model_data %>%
  filter(response_id %in% sample(IDs,round(0.8*length(IDs),0)))
test_model_data = model_data %>%
  filter(!(response_id %in% sample(IDs,round(0.8*length(IDs),0))))
# str(model_data2$duration)
```

I splitted the entire experiment data into training and testing set, by randomly select 80% of the participants into training dataset and the remaining 20% of the participants to the testing dataset. This is equivalent to a 1-folder validation, where I can train the model on the training set and evaluate the performance of the model on the testing set. 

## Multinomial Regression
Multinomial logistic regression is used to model nominal outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables. It's a extension of binary logistic regression that allows for more than two categories for the dependent variable. Similar to binary logistic regression, it uses maximum likelihood estimation to evaluate the probability of belonging to a specific category. 

However, multinomial regression does not address the ordinal structure of the response variable in the data. Therefore, it can serve as a baseline model to compare the performance with ordinal logistic regression model (presented in the next section). 

Mathematically, with p number of independent variables and response variable Y with categories j = 1,...,J, the algorithm selects 1 response category as base level and form odds of remaining J-1 categories against this level: 

$$log(\frac{\pi_{ij}}{\pi_{iJ}}) = \alpha_{j}+x_i'\beta_{j}\\ 
where \\
\pi_{ij} = P(Y_i = j) \\
\alpha_j = \text{intercept} \\
\beta_j \text{ is a vector of regression coefficients for j=1,2,...,Jâˆ’1}$$
```{r, echo = FALSE}

mlr <- multinom(answer ~ duration + offer + outcome + price+rtb+social_proof, 
          data = train_model_data, Hess=TRUE)
## view a summary of the model
summary(mlr) # AIC=22248.33
ctable <- coef(summary(mlr))

## calculate and store p values
#p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2

z <- summary(mlr)$coefficients/summary(mlr)$standard.errors
# 2-tailed Wald z tests to test significance of coefficients
p <- (1 - pnorm(abs(z), 0, 1)) * 2

## combined table
# ctable <- cbind(ctable, "p value" = p)

# CI
# ci <- confint(m2)
# ci

## OR and CI
# exp(cbind(OR = coef(m2), ci))

# performance evaluation
Phat = predict(mlr, newdata = test_model_data, type="p") %>% 
  as.data.frame()
mlr_predictions = colnames(Phat)[apply(Phat,1,which.max)] %>% 
  cbind(test_model_data$answer) %>% 
  as.data.frame() %>% 
  rename("prediction" = ".", "actual"="V2") %>% 
  mutate(match = ifelse(prediction == actual,1,0))
sum(mlr_predictions$match)/nrow(mlr_predictions) # 0.4274345 (0.4714419: 3-level): proportion of correct pred on test set
```


**Interpretation of the model results:**

By looking at the coefficients of the fitted multinomial logistic regression model, I noticed that the base level of the response is answer = 1/Very Likely. Therefore: 

* Duration:
  + The log odds of being in rating 2 vs. in rating 1 will change by `r ctable[1,2]` when comparing duration="6 months" to duration="3 months". 
  + The log odds of being in rating 3 vs. in rating 1 will change by `r ctable[2,2]` when comparing duration="6 months" to duration="3 months". 
  + The log odds of being in rating 4 vs. in rating 1 will change by `r ctable[3,2]` when comparing duration="6 months" to duration="3 months". 
  + The log odds of being in rating 2 vs. in rating 1 will change by `r ctable[1,3]` when comparing duration="12 months" to duration="3 months". 
  + The log odds of being in rating 3 vs. in rating 1 will change by `r ctable[2,3]` when comparing duration="12 months" to duration="3 months". 
  + The log odds of being in rating 4 vs. in rating 1 will change by `r ctable[3,3]` when comparing duration="12 months" to duration="3 months". 
  
* Price:
  + The log odds of being in rating 2 vs. in rating 1 will change by `r ctable[1,10]` when comparing price="\$30/month" to price="\$20/month". 
  + The log odds of being in rating 3 vs. in rating 1 will change by `r ctable[2,10]` when comparing price="\$30/month" to price="\$20/month".  
  + The log odds of being in rating 4 vs. in rating 1 will change by `r ctable[3,10]` when comparing price="\$30/month" to price="\$20/month".  
  + The log odds of being in rating 2 vs. in rating 1 will change by `r ctable[1,11]` when comparing price="\$40/month" to price="\$20/month".   
  + The log odds of being in rating 3 vs. in rating 1 will change by `r ctable[2,11]` when comparing price="\$40/month" to price="\$20/month".   
  + The log odds of being in rating 4 vs. in rating 1 will change by `r ctable[3,11]` when comparing price="\$40/month" to price="\$20/month".  
  
Similar interpretation can be generated for the remaining of the predictors. The baseline level for each categorical variables are the following:
```{r}
data.frame(
  Predictors = c("duration","offer","outcome","price","rtb","social_proof"),
  Baseline_Level = c("3 months","give you the energy to unlock your fullest potential",
                     "breaking bad habits and creating new routines","$20/month",
                     "a program created just for you","a method that has helped thousands")
)

```
**Model Performance:**

Confusion matrix of the multinomial model is presented below, along with out-of-sample accuracy, precision, recall and F1 score: 
```{r, echo = FALSE}
conf_mat <- confusion_matrix(targets = mlr_predictions$actual,
                             predictions = mlr_predictions$prediction)
plot_confusion_matrix(conf_mat$`Confusion Matrix`[[1]],
                      add_sums = TRUE)
print("accuracy")
GradientMatricsSubmission::model_performance_metrics(predictions = mlr_predictions$prediction,
                          targets = mlr_predictions$actual)[[1]] # accuracy:0.4714419
print("precision")
GradientMatricsSubmission::model_performance_metrics(predictions = mlr_predictions$prediction,
                          targets = mlr_predictions$actual)[[2]] # precision
print("recall")
GradientMatricsSubmission::model_performance_metrics(predictions = mlr_predictions$prediction,
                          targets = mlr_predictions$actual)[[3]] # recall
print("F1 score")
GradientMatricsSubmission::model_performance_metrics(predictions = mlr_predictions$prediction,
                          targets = mlr_predictions$actual)[[4]] # f1
```

As we can see, the model has little power in distinguishing likelihood of donwload based on message attributes using a multinomial model. More details is discussed in Section Further Improvement. 

# Ordinal Logistic Regression Model
Ordinal logistic regression is used to determine the relationship between a set of predictors and an ordered factor dependent variable (such as rating). In ordinal logistic regression, the levels within the dependent variable is assumed to have a ordinal features, such as rating 1 is one-step higher than rating 2, and it's 2-step higher than rating 3. In contrast to multinomial regression, where there is NO ordinal structure for the levels within the dependent variable (such as y %in% c("apple","orange","pear")).

$$logit[P(Y \leq j)] = \alpha_j - \beta x, j = 1 ... J-1$$


$$Thus,P(Y \leq j) = \frac{exp(\alpha_j - \beta x)}{1+exp(\alpha_j - \beta x)}, j = 1 ... J-1$$
where $\alpha_j$ is the threshold coefficient corresponding to the particular rating, $\beta$ is the variable coefficient corresponding to a change in a predictor variable, and $x$ is the value of the predictor variable. 

```{r, echo = FALSE}
olr <- polr(answer ~ duration + offer + outcome + price+rtb+social_proof, 
          data = train_model_data, Hess=TRUE)
summary(olr)# AIC: 22225.68 

ctable <- coef(summary(olr))
## calculate and store p values
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2

## combined table
ctable <- cbind(ctable, "p value" = p)


# CI
# ci <- confint(m)
# ci
# 
# ## OR and CI
# exp(cbind(OR = coef(m), ci))

# performance evaluation
Phat = predict(olr, newdata = test_model_data, type="p") %>% 
  as.data.frame()
olr_predictions = colnames(Phat)[apply(Phat,1,which.max)] %>% 
  cbind(test_model_data$answer) %>% 
  as.data.frame() %>% 
  rename("prediction" = ".", "actual"="V2") %>% 
  mutate(match = ifelse(prediction == actual,1,0))
sum(olr_predictions$match)/nrow(olr_predictions) # 0.4274345 (0.4639513 3-level): proportion of correct pred on test set

```

**Interpretation of the model results:**

* Duration:
  + Compared to duration = 3 months, 6-month duration will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[1,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1
  + Compared to duration = 3 months, 12-month duration will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[2,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1

* Price:
  + Compared to price = \$20/month, \$30/month will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[9,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1
  + Compared to price = \$20/month, \$40/month will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[10,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1

Similar interpretation can be generated for the remaining of the predictors. The baseline level for each categorical variables are the following:
```{r}
data.frame(
  Predictors = c("duration","offer","outcome","price","rtb","social_proof"),
  Baseline_Level = c("3 months","give you the energy to unlock your fullest potential",
                     "breaking bad habits and creating new routines","$20/month",
                     "a program created just for you","a method that has helped thousands")
)

```

**Model Performance:**
Confusion matrix of the multinomial model is presented below, along with out-of-sample accuracy, precision, recall and F1 score: 
```{r}
conf_mat <- confusion_matrix(targets = olr_predictions$actual,
                             predictions = olr_predictions$prediction)
plot_confusion_matrix(conf_mat$`Confusion Matrix`[[1]],
                      add_sums = TRUE)
print("Accuracy")
GradientMatricsSubmission::model_performance_metrics(predictions = olr_predictions$prediction,
                          targets = olr_predictions$actual)[[1]] # accuracy:0.4274345
print("Precision")
GradientMatricsSubmission::model_performance_metrics(predictions = olr_predictions$prediction,
                          targets = olr_predictions$actual)[[2]] # precision
print("Recall")
GradientMatricsSubmission::model_performance_metrics(predictions = olr_predictions$prediction,
                          targets = olr_predictions$actual)[[3]] # recall
print("F1 Score")
GradientMatricsSubmission::model_performance_metrics(predictions = olr_predictions$prediction,
                          targets = olr_predictions$actual)[[4]] # f1
```
Similar to multinomial regression model, the ordinal model has little power in distinguishing likelihood of donwload based on message attributes. Possible explanations include the unbalanced distribution of the rating, outlined below, where more than half the participants gave rating = 1 in their answers.
```{r, echo= FALSE}
table(exp_data$answer)
```

Another reason is that the ability of each predictor to distinguish different rating levels is quite weak. As we can see in the EDA document, the distributions of rating within each attribute are very similar to each other (seen in Section Experiment Data in EAD vignettes). For instance, the proportion of rating = Very Likely is around 43% across all durations (3-, 6- & 12-month), the proportion of rating = Somewhat Likely is around 21% across all durations (3-, 6- & 12-month), the proportion of rating = Somewhat Unlikely is around 20% across all durations (3-, 6- & 12-month), and the proportion of rating = Very Unlikely is around 15% across all durations (3-, 6- & 12-month).
```{r, echo=FALSE}
data("experiment_data")

employment_age = GradientMatricsSubmission::dist_bar_chart_dataPrep(data= exp_data,
                        grouping_var = "duration",
                        index_var = "answer") %>% 
  mutate(grouping_var2 = grouping_var)
GradientMatricsSubmission::dist_bar_chart(data = employment_age,
               index_var_names = "Duration ",
               index_var_levels = c("Very Likely","Somewhat Likely","Somewhat Unlikely","Very Unlikely"),
               title = "Rating Distribution by duration")
```


# Ordinal Mixed Effect Model
Given the fact that each participants were randomly given 12 permutations of the message attributes, each participants has 12 observations, each corresponding to his/her rating for download likelihood. The previous 2 models did NOT account for the random effect within the data. Ordinal mixed effect model (specifically, cumulative link mixed model) can address this issue. 


The  assumption  of  independence  of  observations  applies  to  all  regression  models. When multiple measurements are taken on the same individual, this assumption is violated. In order to account for dependent observations, a random effect can be added to the previous model. Cumulative link mixed models have the following general form:

$$
logit[P(Y_i \leq j)] = \alpha_j - (Z_{t[i]}u_t + x_i\beta) \\
\text{where } u_t \sim N(0,\sigma_u^2)
$$

$u_t$ represents the vector of coefficients corresponding to the group-level predictors $Z_{t[i]}$ for observation $i$ in cluster/participant $t$. This model has the added assumption that the random effects are Normally distributed and centered at zero. The random effect induces the correlation expected between observations in the same cluster/participants. 

Since the experiment data contains `r length(unique(survey_data$response_id))` participants, when I used response_id as random effect variable, the model could not converge (convergence is needed in order to produce the Hessian matrix & corresponding standard error for each estimated coefficient). 

Therefore, I used a clustering technique to first group participants into different clusters using social-demographic features from Survey Data. And assign random effect variable to this cluster variable in the ordinal mixed effect model. 

```{r}
library(poLCA)
survey_data3 = survey_data
survey_data3[is.na(survey_data3)] <- 0

# f1 = as.formula(cbind(d_urban,s_gender,s_race,d_education,s_hhincome,s_problem)~1)
f2 = as.formula(cbind(source_1,source_4,source_5,source_6,source_7,source_8,source_9,source_10,source_11,source_12,source_13,source_14,source_15,source_16,source_17)~1)

source_df = survey_data3 %>% dplyr::select(contains("source"))
source_df[(source_df==1)] <- 2
source_df[(source_df==0)] <- 1
LCA1 = poLCA(f2,data = source_df,nclass=3,nrep = 10,verbose = FALSE)
plot(LCA1)

survey_data3$cluster_source = LCA1$predclass
train_model_data2 = train_model_data %>% 
  left_join(survey_data3 %>% 
  dplyr::select(response_id,cluster_source),on = c("response_id"="response_id")) %>% 
  mutate(duration = factor(duration,levels = c("3 months","6 months","12 months")),
    price = factor(price,levels = c("$20/month","$30/month","$40/month"))
  )
test_model_data2 = test_model_data %>% 
  left_join(survey_data3 %>% 
  dplyr::select(response_id,cluster_source),on = c("response_id"="response_id")) %>% 
  mutate(duration = factor(duration,levels = c("3 months","6 months","12 months")),
    price = factor(price,levels = c("$20/month","$30/month","$40/month"))
  )

# fitLCA <- function(k){
#   poLCA(cbind(m1_philosophy_1, m1_philosophy_2, m1_philosophy_3,
#                    m1_philosophy_4, m1_philosophy_5, m1_philosophy_6,
#                    m1_philosophy_7, m1_philosophy_8, m1_philosophy_9)~1, 
#         data=survey_data, nclass = k, nrep=10)
# }
# # Apply the function to successively increasingly classes K=1,2,3....,6
# MK <- lapply(1:6, fitLCA)
# # Possible to look at AIC, BIC, etc.
# sapply(MK, `[[`, "aic")
# sapply(MK, `[[`, "bic")

```

As we can see from the visualization above, for the 3 clusters the model defined: 

* Given participant is in $1^{st}$ cluster (left column), the probabilities of him/her having BOTH physical (source 5: a chronic condition (e.g. cancer, diabetes, cardiovascular disease etc.)) and psychological sources (Source 8: Stress from current events (e.g. pandemic, politics)) of sleep issues are quite high. 

* Given a participant is in $2^{nd}$ cluster, he/she is characterized by high probabilities of reporting Other sources of sleep issues (a.k.a. not listed in the survey)

* Given a participant is in $3^{rd}$ cluster, he/she is more likely to have psychological sources of sleep issues (source 7,8,9 which are mostly chronic anxiety, depression from both personal life and current events)

The following cumulative link mixed effect uses all 6 message attributes (duration, offer, outcome etc.) as fixed effect variable and uses the source cluster as random effect variable. 

```{r}
library(ordinal)
model7 = clmm(answer~duration+offer+outcome+price+rtb+social_proof+(1|cluster_source),
               link='logit',
               data= train_model_data2,
               Hess=TRUE, nAGQ=1)
summary(model7) # AIC = 21757.87
ctable = coef(summary(model7))
ctable
```

**Interpretation of the model results:**

* Duration: by taking the effect of sleep issue source clusters into account as random effect
  + Compared to duration = 3 months, 6-month duration will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[1,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1
  + Compared to duration = 3 months, 12-month duration will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[2,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1

* Price: by taking the effect of sleep issue source clusters into account as random effect
  + Compared to price = \$20/month, \$30/month will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[9,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1
  + Compared to price = \$20/month, \$40/month will cause the likelihood 
  of a rating 4 vs. a rating 1-3 change by `r ctable[10,1]` on the log odds scale, 
  and same amount of changes will happen to the likelihood of a rating 3 vs. a rating 1-2 \& the likelihood of a rating 2 vs. a rating 1

Similar interpretation can be generated for the remaining of the predictors. The baseline level for each categorical variables are the following:
```{r}
data.frame(
  Predictors = c("duration","offer","outcome","price","rtb","social_proof"),
  Baseline_Level = c("3 months","give you the energy to unlock your fullest potential",
                     "breaking bad habits and creating new routines","$20/month",
                     "a program created just for you","a method that has helped thousands")
)

```

**Overall, the model performance of all 3 models are summarized below: the ordinal mixed effect model using source cluster as random effect variable yields the smallest number of in-sample AIC.**

```{r}
data.frame(
  model = c("Multinomial Logistic Regression", "Ordinal Logistic Regression", "Ordinal Mixed Effect Model"),
  AIC = c(round(summary(mlr)$AIC,2),
          round(olr$deviance,2),
          summary(model7)$info["AIC"] %>% pull())
  
)

```

## Further Improvement
Ideally, a K-folder cross validation should be use to train, validate and test the model, where out-of-sample performance metrics can be calculated to evaluate the accuracy of the predictions.

Group Lasso

## Reference
https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/
https://data.princeton.edu/wws509/notes/c6.pdf
https://math.montana.edu/grad_students/writing-projects/2012/12schmidt.pdf
